Roast Evaluation Judge — Model Architecture & Technical Overview

Model Purpose and Scope

The Roast Evaluation Judge is a transformer-based neural scoring model designed to objectively evaluate conversational roast battles between two participants, denoted as Speaker A and Speaker B. Unlike generative models that produce dialogue, this system is purpose-built for comparative judgment, assigning interpretable scores that reflect comedic and rhetorical quality across multiple dimensions. The model is optimized for stability, consistency, and real-time evaluation, making it suitable for deployment as an API-backed judge in AI-versus-AI or Human-versus-AI roast battles.

⸻

Core Architecture

At its foundation, the model uses a pretrained encoder-only Transformer, following a RoBERTa-style architecture. This transformer serves as a contextual feature extractor, converting the full conversational context into dense semantic representations. The model does not rely on token-level generation; instead, it leverages the transformer’s ability to holistically encode discourse, tone, and stylistic nuance across multiple conversational turns.

The encoder outputs a sequence of hidden states, from which the [CLS] token representation is extracted. This pooled embedding functions as a compact summary of the entire roast interaction and serves as the sole input to the downstream scoring head.

⸻

Input Representation

The evaluation input is constructed in a structured textual format that mirrors the data distribution used during training. Specifically, the model receives:
	•	The entire battle thread (multi-turn conversation with speaker labels)
	•	A concatenation of all utterances by Speaker A
	•	A concatenation of all utterances by Speaker B

These components are combined into a single input sequence using explicit textual separators (e.g., THREAD, A_LAST, B_LAST) to preserve speaker roles and contextual boundaries. The resulting sequence is tokenized using the transformer’s native tokenizer and truncated or padded to a fixed maximum length, ensuring consistent computational performance.

⸻

Scoring Head and Outputs

On top of the transformer encoder sits a lightweight feed-forward regression head, implemented as a small multilayer perceptron (MLP). This head maps the pooled transformer embedding to eight continuous outputs, corresponding to four evaluation dimensions for each speaker:
	•	Humor
	•	Punch
	•	Originality
	•	Relevance

Each of these subscores is predicted independently as a real-valued regression output. To improve interpretability and downstream usability, the raw values are passed through a sigmoid-based normalization, mapping them onto a 0–100 scale.

⸻

Aggregation and Winner Determination

For each speaker, an overall score is computed as the arithmetic mean of their four subscores. The model then compares the overall scores of Speaker A and Speaker B to determine the outcome of the battle. A configurable tie threshold is applied to prevent overconfident decisions in marginal cases:
	•	If the score difference falls within the threshold, the result is labeled as TIE
	•	Otherwise, the speaker with the higher overall score is declared the winner

This logic ensures that the model behaves conservatively when differences are statistically or stylistically ambiguous.

⸻

Design Philosophy and Extensibility

The Roast Evaluation Judge is explicitly designed as a comparative evaluation model, not a conversational agent. This separation of concerns allows it to remain:
	•	Deterministic and stable across runs
	•	Fast enough for real-time inference
	•	Robust to stylistic variation across speakers

The architecture is fully compatible with retraining or fine-tuning using human feedback, making it straightforward to incorporate new judgment criteria, rebalance scoring dimensions, or adapt to different comedic styles without altering the inference API or deployment pipeline.

⸻

If you want, next I can:
	•	Convert this into a full academic-style “Model” section
	•	Write a Training Methodology section to complement this
	•	Or produce a Lovable-compatible evaluator prompt that aligns exactly with this model’s philosophy



